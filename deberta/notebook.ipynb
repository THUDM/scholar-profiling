{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/shishijie/anaconda3/envs/deberta/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel,AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptm_path='/home/shishijie/workspace/PTMs/deberta-base'\n",
    "tokenizer=AutoTokenizer.from_pretrained(ptm_path)\n",
    "model=AutoModel.from_pretrained(ptm_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='你好'\n",
    "tokenized_data=tokenizer(text,return_tensors='pt')\n",
    "outputs=model(**tokenized_data,output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutput(last_hidden_state=tensor([[[ 0.0452, -0.0113, -0.0761,  ...,  0.0363,  0.0723, -0.0534],\n",
      "         [-1.2469,  0.1110,  0.6634,  ...,  1.7498, -1.2915,  0.4342],\n",
      "         [-0.1360, -0.8177,  0.6069,  ...,  2.2420, -0.6911,  0.5334],\n",
      "         [ 0.5248,  0.2118,  1.0589,  ...,  1.5573, -1.1657,  0.4546],\n",
      "         [ 0.2990, -0.2704,  0.4432,  ...,  2.2601, -0.6226,  0.5809],\n",
      "         [ 0.2197,  0.0697, -0.1574,  ...,  0.0859,  0.1927,  0.0920]]],\n",
      "       grad_fn=<AddBackward0>), hidden_states=(tensor([[[-0.3259,  0.3926, -0.5905,  ..., -0.2977, -0.1261,  0.2434],\n",
      "         [-1.1396, -0.7813, -0.6273,  ...,  0.4993,  0.0726, -0.4027],\n",
      "         [-1.2837, -0.6408,  0.4874,  ...,  1.8429,  0.5336,  1.1886],\n",
      "         [-0.7672,  0.4678, -0.4241,  ...,  0.3074, -0.5668,  0.3768],\n",
      "         [ 0.0948, -0.6262, -0.6328,  ..., -1.2387, -1.4035,  0.1604],\n",
      "         [-0.3745,  0.7429, -0.1807,  ..., -0.4511, -0.5262,  0.4496]]],\n",
      "       grad_fn=<MulBackward0>), tensor([[[-0.1885,  0.2611,  0.0562,  ...,  0.5551, -0.5316, -0.3942],\n",
      "         [-1.2944, -0.2409,  0.2296,  ...,  1.3008, -0.3228, -1.2256],\n",
      "         [-0.7480, -0.4982,  0.4628,  ...,  2.9562,  0.2115,  0.5104],\n",
      "         [-0.4963,  0.3707,  0.6351,  ...,  1.6133, -0.2238, -0.5682],\n",
      "         [-0.0592, -0.5829,  0.0520,  ...,  0.8135, -0.9907, -0.3889],\n",
      "         [-0.2779,  0.2747,  0.1635,  ...,  0.3937, -0.2108, -0.6902]]],\n",
      "       grad_fn=<AddBackward0>), tensor([[[ 6.2042e-02, -3.4054e-01,  7.9471e-03,  ...,  2.0420e-01,\n",
      "          -9.1715e-01, -1.7889e-01],\n",
      "         [-1.0075e+00,  1.0648e-01, -2.8071e-01,  ...,  1.0558e+00,\n",
      "          -2.1847e-01, -2.7068e-01],\n",
      "         [-2.6305e-01, -7.0390e-01,  2.2816e-01,  ...,  2.5021e+00,\n",
      "          -5.8739e-01,  4.6105e-01],\n",
      "         [-3.4465e-01, -8.4677e-02,  6.3277e-01,  ...,  1.5135e+00,\n",
      "          -4.9094e-01, -2.9231e-01],\n",
      "         [ 3.3160e-01, -3.8191e-01, -1.5028e-01,  ...,  1.1829e+00,\n",
      "          -4.9036e-01, -1.3838e-01],\n",
      "         [-2.5834e-02, -5.5303e-03, -1.2198e-02,  ..., -1.6126e-02,\n",
      "           8.4125e-04, -5.4628e-02]]], grad_fn=<AddBackward0>), tensor([[[-0.2172, -0.1389, -0.0599,  ..., -0.1371, -1.1560, -0.1106],\n",
      "         [-1.5232, -0.0599, -0.6039,  ...,  0.7918, -0.9903,  0.5475],\n",
      "         [-0.5866, -1.0313,  0.1628,  ...,  1.7894, -1.2706,  0.7477],\n",
      "         [-0.2198, -0.1381,  0.7157,  ...,  1.5590, -0.7837,  0.3677],\n",
      "         [ 0.2964, -0.5411, -0.1011,  ...,  1.2954, -1.2944,  0.3746],\n",
      "         [-0.0568,  0.0141, -0.0283,  ...,  0.0086,  0.0377, -0.0575]]],\n",
      "       grad_fn=<AddBackward0>), tensor([[[ 2.8352e-02, -2.1773e-01,  1.7827e-01,  ..., -2.4528e-01,\n",
      "          -1.3441e+00, -7.1268e-01],\n",
      "         [-1.8046e+00, -1.9640e-02,  1.2407e-01,  ...,  1.0118e+00,\n",
      "          -1.3118e+00,  3.9760e-02],\n",
      "         [-5.5341e-01, -1.3774e+00,  6.1974e-01,  ...,  1.4561e+00,\n",
      "          -1.2460e+00,  7.9380e-01],\n",
      "         [-8.0744e-01, -3.9548e-01,  1.3178e+00,  ...,  1.5988e+00,\n",
      "          -8.1771e-01, -1.4393e-02],\n",
      "         [ 1.0440e-01, -9.2684e-01,  6.7657e-01,  ...,  1.2456e+00,\n",
      "          -1.4018e+00,  3.1460e-01],\n",
      "         [ 2.4768e-02,  1.4710e-02,  4.6253e-04,  ..., -4.1837e-02,\n",
      "           6.4433e-02, -2.7204e-03]]], grad_fn=<AddBackward0>), tensor([[[ 0.3770, -0.1732,  0.5289,  ...,  0.2807, -1.6784, -0.2978],\n",
      "         [-1.6244,  0.3636,  0.1529,  ...,  0.8884, -2.1541,  0.3317],\n",
      "         [-0.1653, -1.1880,  0.0636,  ...,  1.0683, -1.5435,  0.5992],\n",
      "         [-0.2901, -0.3791,  1.0680,  ...,  1.5429, -1.3463,  0.3353],\n",
      "         [ 0.6339, -0.6984,  0.9203,  ...,  0.8323, -1.6407,  0.8273],\n",
      "         [ 0.0661, -0.0053,  0.0249,  ..., -0.0566,  0.0510,  0.0432]]],\n",
      "       grad_fn=<AddBackward0>), tensor([[[-0.0302, -0.2102,  0.0110,  ...,  0.4111, -1.0993, -0.1429],\n",
      "         [-2.0216,  0.0924,  0.5358,  ...,  1.3972, -1.9822,  0.5095],\n",
      "         [-0.5421, -1.1029,  0.1248,  ...,  0.8975, -1.4608,  0.9051],\n",
      "         [-0.5390, -0.0537,  0.6618,  ...,  1.4206, -1.0567, -0.3093],\n",
      "         [ 0.3118, -0.1613,  0.6407,  ...,  1.2746, -1.2220,  0.2205],\n",
      "         [ 0.0665, -0.0040,  0.0234,  ..., -0.0244,  0.0778, -0.0296]]],\n",
      "       grad_fn=<AddBackward0>), tensor([[[-0.0238, -0.1080,  0.3094,  ...,  0.7593, -1.5105, -0.2251],\n",
      "         [-2.0980, -0.0462,  0.6070,  ...,  1.7138, -2.2710,  0.0506],\n",
      "         [-0.5394, -1.4268,  0.5362,  ...,  1.2847, -1.4816,  0.4733],\n",
      "         [-0.5916, -0.4990,  0.5580,  ...,  1.9022, -1.3416, -0.0316],\n",
      "         [ 0.2665, -0.4762,  0.7033,  ...,  1.7862, -1.6236,  0.6149],\n",
      "         [ 0.0585, -0.0037,  0.0197,  ..., -0.0230,  0.0480,  0.0090]]],\n",
      "       grad_fn=<AddBackward0>), tensor([[[ 0.3602, -0.0260, -0.6953,  ...,  0.8843, -1.2790,  0.1507],\n",
      "         [-2.1599, -0.2016,  0.5953,  ...,  2.0487, -2.2978,  0.3998],\n",
      "         [-0.5365, -1.5656,  0.2969,  ...,  1.8792, -1.5154,  0.8106],\n",
      "         [-0.2615, -0.5523,  0.3757,  ...,  1.7242, -1.5741,  0.0757],\n",
      "         [ 0.2234, -0.8458,  0.3963,  ...,  1.8688, -1.2239,  0.9202],\n",
      "         [ 0.0419, -0.0103,  0.0177,  ..., -0.0355,  0.0406,  0.0154]]],\n",
      "       grad_fn=<AddBackward0>), tensor([[[-0.0655, -0.2651, -0.2147,  ...,  0.7019, -1.3865,  0.4899],\n",
      "         [-2.1543, -0.2395,  0.3212,  ...,  2.0729, -1.7806,  0.4362],\n",
      "         [-0.7328, -1.7639,  0.2666,  ...,  1.8740, -0.7641,  0.9721],\n",
      "         [-0.0595, -0.2432,  0.5630,  ...,  1.5668, -1.6095,  0.2240],\n",
      "         [-0.0566, -0.9725,  0.3327,  ...,  1.7154, -1.3145,  0.7382],\n",
      "         [ 0.0469, -0.0171,  0.0154,  ..., -0.0293,  0.0251,  0.0227]]],\n",
      "       grad_fn=<AddBackward0>), tensor([[[-0.2807, -0.2333, -0.4566,  ...,  1.1754, -1.4453,  0.1932],\n",
      "         [-2.3974, -0.2303, -0.1849,  ...,  2.2216, -1.5082,  0.4836],\n",
      "         [-0.7259, -1.8304, -0.3026,  ...,  2.3111, -0.2410,  0.8600],\n",
      "         [-0.0521, -0.1321,  0.4720,  ...,  1.7519, -1.2276,  0.4076],\n",
      "         [-0.2120, -0.7122, -0.1387,  ...,  2.1840, -0.7653,  0.3189],\n",
      "         [ 0.0052, -0.0150,  0.0402,  ..., -0.0068,  0.0404,  0.0100]]],\n",
      "       grad_fn=<AddBackward0>), tensor([[[-0.4002, -0.4303, -0.4404,  ...,  0.5321, -0.8544,  0.0041],\n",
      "         [-2.0357, -0.3420,  0.0473,  ...,  1.8030, -1.3191,  0.1412],\n",
      "         [-0.6352, -1.7996,  0.2050,  ...,  1.9071, -0.0216,  0.2661],\n",
      "         [ 0.1281, -0.1883,  0.6380,  ...,  1.3283, -1.2306, -0.0349],\n",
      "         [ 0.0331, -0.7311,  0.0645,  ...,  1.7419, -0.8391, -0.0570],\n",
      "         [ 0.0406, -0.0072,  0.0209,  ..., -0.0071,  0.0310,  0.0110]]],\n",
      "       grad_fn=<AddBackward0>), tensor([[[ 0.0452, -0.0113, -0.0761,  ...,  0.0363,  0.0723, -0.0534],\n",
      "         [-1.2469,  0.1110,  0.6634,  ...,  1.7498, -1.2915,  0.4342],\n",
      "         [-0.1360, -0.8177,  0.6069,  ...,  2.2420, -0.6911,  0.5334],\n",
      "         [ 0.5248,  0.2118,  1.0589,  ...,  1.5573, -1.1657,  0.4546],\n",
      "         [ 0.2990, -0.2704,  0.4432,  ...,  2.2601, -0.6226,  0.5809],\n",
      "         [ 0.2197,  0.0697, -0.1574,  ...,  0.0859,  0.1927,  0.0920]]],\n",
      "       grad_fn=<AddBackward0>)), attentions=None)\n"
     ]
    }
   ],
   "source": [
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 138601728\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of parameters:\", sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 2, 3, 4]],\n",
      "\n",
      "        [[5, 6, 7, 8]]])\n",
      "[tensor([[1, 2, 3, 4]]), tensor([[5, 6, 7, 8]]), tensor([[11, 22, 33, 44]]), tensor([[55, 66, 77, 88]])]\n",
      "tensor([[ 1,  2,  3,  4,  5,  6,  7,  8, 11, 22, 33, 44, 55, 66, 77, 88]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([432])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ten1=torch.tensor([[[1,2,3,4]],[[5,6,7,8]]])\n",
    "ten2=torch.tensor([[[11,22,33,44]],[[55,66,77,88]]])\n",
    "print(ten1)\n",
    "ten=[]\n",
    "ten+=ten1\n",
    "ten+=ten2\n",
    "print(ten)\n",
    "ten=torch.cat(ten,dim=1)\n",
    "print(ten)\n",
    "ten=torch.sum(ten,dim=1)\n",
    "ten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[39mprint\u001b[39m(sc)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# with open('./data/y_pred.json','r') as fr:\n",
    "#     y_pred=json.loads(fr.read()) \n",
    "\n",
    "# with open('./data/y_true.json','r') as fr:\n",
    "#     y_true=json.loads(fr.read()) \n",
    "    \n",
    "y_true = np.array([[1, 0, 0, 0],\n",
    "                   [0, 0, 0, 1],\n",
    "                   [0, 1, 0, 0],\n",
    "                   [0, 0, 0, 1],\n",
    "                   [0, 0, 0, 1],\n",
    "                   [0, 0, 0, 1]])\n",
    "y_pred = np.array([[1, 0, 0, 0],\n",
    "                    [0, 0, 0, 1],\n",
    "                    [0, 0, 0, 1],\n",
    "                    [1, 0, 0, 1],\n",
    "                    [0, 0, 0, 1],\n",
    "                    [0, 0, 0, 1]])\n",
    "try:\n",
    "    sc=roc_auc_score(y_true, y_pred) ## y_true=ground_truth\n",
    "except ValueError:\n",
    "    pass\n",
    "print(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('/home/shishijie/workspace/project/scholar/data/chatglm_input/homepage_test_with_label.json','r',encoding='utf8') as fr:\n",
    "    data=json.loads(fr.read())\n",
    "\n",
    "homepage_data=[]\n",
    "for item in data:\n",
    "    name=item['instruction'][11:-6]\n",
    "    text=item['input'].strip().replace('\\n','').replace('\\r','').replace('\\t','')\n",
    "    if '不是个人主页' in item['output']:\n",
    "        label='0'\n",
    "    else:\n",
    "        label='1'\n",
    "    homepage_data.append(name+'[SEP]'+text+'\\t'+label)\n",
    "\n",
    "with open('./data/homepage.test','w',encoding='utf8') as fw:\n",
    "    for item in homepage_data:\n",
    "        fw.write(item+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer,AutoModel,AutoConfig\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from tqdm import tqdm\n",
    "# 数据处理\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self,data_path,ptm_path,max_length=512) -> None:\n",
    "        super().__init__()\n",
    "        self.tokenizer=AutoTokenizer.from_pretrained(ptm_path)\n",
    "        \n",
    "        self.data,self.label=self.get_data(data_path)\n",
    "        self.label=torch.LongTensor(self.label)\n",
    "        self.length=len(self.data)\n",
    "        self.tokenized_data=self.tokenize_data(self.tokenizer,self.data,max_length)\n",
    "\n",
    "    def get_data(self,path):\n",
    "        data=[]\n",
    "        label=[]\n",
    "        with open(path,'r',encoding='utf8') as fr:\n",
    "            for line in fr.readlines():\n",
    "                line=line.strip().split('\\t')\n",
    "                data.append(line[0])\n",
    "                label.append(int(line[1]))\n",
    "        return data,label\n",
    "    \n",
    "    def tokenize_data(self,tokenizer,data,max_length):\n",
    "        tokenizer_data=[]\n",
    "        for item in tqdm(data,desc='Tokenizing'):\n",
    "            tokenizer_data.append(\n",
    "                tokenizer(item,truncation=True,padding='max_length',max_length=max_length,return_tensors='pt')\n",
    "            )\n",
    "        return tokenizer_data\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, index) :\n",
    "        input=self.tokenized_data[index]\n",
    "        label=self.label[index]\n",
    "        \n",
    "        for k,v in input.items():\n",
    "            input[k]=v.squeeze(0)\n",
    "        \n",
    "        \n",
    "        return input,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing:   0%|          | 0/1831 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing: 100%|██████████| 1831/1831 [00:07<00:00, 245.96it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset=MyDataset('/home/shishijie/workspace/project/deberta_cls/data/gender.test','/home/shishijie/workspace/PTMs/albert-base-v2',128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.MyDataset"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.81      0.77       334\n",
      "           1       0.73      0.72      0.72       236\n",
      "           2       0.59      0.63      0.61       213\n",
      "           3       0.62      0.43      0.51        37\n",
      "           4       0.80      0.50      0.62        16\n",
      "           5       0.00      0.00      0.00         8\n",
      "           6       1.00      0.14      0.25         7\n",
      "           7       0.67      0.57      0.62         7\n",
      "           8       0.00      0.00      0.00         6\n",
      "           9       0.00      0.00      0.00         3\n",
      "          10       1.00      0.33      0.50         3\n",
      "          11       0.00      0.00      0.00         2\n",
      "          12       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.69       873\n",
      "   macro avg       0.47      0.32      0.35       873\n",
      "weighted avg       0.68      0.69      0.68       873\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/shishijie/anaconda3/envs/deberta/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/data/shishijie/anaconda3/envs/deberta/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/data/shishijie/anaconda3/envs/deberta/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sklearn.metrics import classification_report\n",
    "text2id={'气滞血瘀':0,'气阴两虚':1,'气虚血瘀':2,'寒凝血瘀':3,'心肾阳虚':4,'心血瘀阻':5,'心肾阴虚':6,'痰浊闭阻':7,'痰阻血瘀':8,'气滞心胸':9,'热毒血瘀':10,'寒凝心脉':11,'心气虚弱':12}\n",
    "pred=[]\n",
    "truth=[]\n",
    "with open('/home/shishijie/workspace/project/ChatGLM-Efficient-Tuning/predict/diagnosis_train_5e-5_full/generated_predictions.jsonl','r',encoding='utf8') as fr:\n",
    "    for item in fr.readlines():\n",
    "        item=json.loads(item)\n",
    "        pred.append(text2id[item['predict'][-4:]])\n",
    "        truth.append(text2id[item['label'][-4:]])\n",
    "\n",
    "report=classification_report(truth,pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deberta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
